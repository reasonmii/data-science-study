{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"08-1. Transformer-based Seq2Seq.ipynb","private_outputs":true,"provenance":[{"file_id":"1gKw5d9kWLnKcCNlaTFpumGn2vvG8BDNq","timestamp":1629634981622},{"file_id":"19-LdpGXlz3ZB0oteEptE-Dq-MmNGVqcz","timestamp":1629218504423}],"collapsed_sections":[],"authorship_tag":"ABX9TyPog9a6XM+Eu2d6sf6BVCRc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"x8DiY3kGR8ql"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","import os\n","import sys\n","from datetime import datetime\n","\n","drive_project_root = \"/content/drive/MyDrive/#fastcampus\"\n","sys.path.append(drive_project_root)\n","!pip install -r \"/content/drive/MyDrive/#fastcampus/requirements.txt\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6U6xAM-ESS6Q"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"phsaYuiZSUOI"},"source":["# For data loading.\n","from typing import List\n","from typing import Dict\n","from typing import Union\n","from typing import Any\n","from typing import Optional\n","from typing import Iterable\n","from typing import Callable\n","from abc import abstractmethod\n","from abc import ABC\n","from datetime import datetime\n","from functools import partial\n","from collections import Counter\n","from collections import OrderedDict\n","import random\n","import math\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.nn import Transformer\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","import pytorch_lightning as pl\n","from pprint import pprint\n","\n","from torchtext import data\n","from torchtext import datasets\n","from torchtext.datasets import Multi30k\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.data.functional import to_map_style_dataset\n","from torchtext.vocab import Vocab, build_vocab_from_iterator, vocab\n","import spacy\n","\n","# For configuration\n","from omegaconf import DictConfig\n","from omegaconf import OmegaConf\n","import hydra\n","from hydra.core.config_store import ConfigStore\n","\n","# For logger\n","from torch.utils.tensorboard import SummaryWriter\n","import wandb\n","os.environ[\"WANDB_START_METHOD\"]=\"thread\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EOMF4OHkSbT8"},"source":["from data_utils import dataset_split\n","from config_utils import flatten_dict\n","from config_utils import register_config\n","from config_utils import configure_optimizers_from_cfg\n","from config_utils import get_loggers\n","from config_utils import get_callbacks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qDx6jCMTTthj"},"source":["# download spacy data.\n","!python -m spacy download en\n","!python -m spacy download en_core_web_sm\n","!python -m spacy download de\n","!python -m spacy download de_core_news_sm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SMcWw01mVjko"},"source":["data_spacy_de_en_cfg = {\n","    \"name\": \"spacy_de_en\",\n","    \"data_root\": os.path.join(os.getcwd(), \"data\"),\n","    \"tokenizer\": \"spacy\",\n","    \"src_lang\": \"de\",\n","    \"tgt_lang\": \"en\",\n","    \"src_index\": 0,\n","    \"tgt_index\": 1,\n","    \"vocab\": {\n","        \"special_symbol2index\": {\n","            \"<unk>\": 0,\n","            \"<pad>\": 1,\n","            \"<bos>\": 2,\n","            \"<eos>\": 3,\n","        },\n","        \"special_first\": True,\n","        \"min_freq\": 2\n","    }\n","}\n","\n","data_cfg = OmegaConf.create(data_spacy_de_en_cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D3Evvm4iWk2L"},"source":["train_data, valid_data, test_data = Multi30k(data_cfg.data_root)\n","test_data = to_map_style_dataset(test_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3mvnHw-tXA-y"},"source":["for i in test_data:\n","    print(i)\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XKi510fHXTJV"},"source":["# 1. token_transform (token ...)\n","\n","def get_token_transform(data_cfg: DictConfig) -> dict:\n","    token_transform: dict = {}\n","    token_transform[data_cfg.src_lang] = get_tokenizer(\n","        data_cfg.tokenizer, language=data_cfg.src_lang\n","    )\n","    token_transform[data_cfg.tgt_lang] = get_tokenizer(\n","        data_cfg.tokenizer, language=data_cfg.tgt_lang\n","    )\n","    return token_transform\n","\n","token_transform = get_token_transform(data_cfg)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtYULy6kYE57"},"source":["# 2. vocab_transform \n","def yield_tokens(\n","    data_iter: Iterable, lang: str, lang2index: Dict[str, int],\n",") -> List[str]:\n","    \"\"\"help function to yield list of tokens\"\"\"\n","    for data_sample in data_iter:\n","        yield token_transform[lang](data_sample[lang2index[lang]])\n","\n","\n","def get_vocab_transform(data_cfg: DictConfig) -> dict:\n","    vocab_transform: dict = {}\n","    for ln in [data_cfg.src_lang, data_cfg.tgt_lang]:\n","        # build from train_data\n","        train_iter = Multi30k(\n","            split=\"train\", language_pair=(data_cfg.src_lang, data_cfg.tgt_lang)\n","        )\n","\n","        # create torchtext's Vocab object.\n","        vocab_transform[ln] = build_vocab_from_iterator(\n","            yield_tokens(\n","                train_iter,\n","                ln,\n","                {\n","                    data_cfg.src_lang: data_cfg.src_index,\n","                    data_cfg.tgt_lang: data_cfg.tgt_index,\n","                }\n","            ),\n","            min_freq=data_cfg.vocab.min_freq,\n","            specials=list(data_cfg.vocab.special_symbol2index.keys()),\n","            special_first=True\n","        )\n","    \n","    # set UNKNOWN as the default index. --> index가 unknown으로 return : token이 찾아지지 않을 경우 !\n","    # 만약 세팅되지 않으면, runtime error 가 날 수 있다 !\n","    for ln in [data_cfg.src_lang, data_cfg.tgt_lang]:\n","        vocab_transform[ln].set_default_index(data_cfg.vocab.special_symbol2index[\"<unk>\"])\n","    \n","    return vocab_transform\n","\n","vocab_transform = get_vocab_transform(data_cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_YNhXn-Wb6pF"},"source":["print(vocab_transform[\"de\"][\"<unk>\"])\n","print(vocab_transform[\"en\"][\"<unk>\"])\n","print(vocab_transform[\"en\"][\"hello\"], vocab_transform[\"en\"][\"world\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dShwyR5XYJnQ"},"source":["# 3 integrated transforms\n","# text_transform: [token_transform --> vocab_transform --> torch.tensor transfrom]\n","\n","\n","# helper function for collate_fn\n","def sequential_transforms(*transforms):\n","    def func(txt_input):\n","        for transform in transforms:\n","            txt_input = transform(txt_input)\n","        return txt_input\n","    return func\n","\n","# convert to torch.tensor with bos & eos\n","def tensor_transform(token_ids: List[int], bos_index: int, eos_index: int):\n","    return torch.cat(\n","        (torch.tensor([bos_index]), torch.tensor(token_ids), torch.tensor([eos_index]))\n","    )\n","\n","# src & tgt lang language text_transforms to convert raw strings --> tensor indices\n","def get_text_transform(data_cfg: DictConfig):\n","    text_transform = {}\n","    for ln in [data_cfg.src_lang, data_cfg.tgt_lang]:\n","        text_transform[ln] = sequential_transforms(\n","            token_transform[ln],\n","            vocab_transform[ln],\n","            partial(\n","                tensor_transform,\n","                bos_index=data_cfg.vocab.special_symbol2index[\"<bos>\"],\n","                eos_index=data_cfg.vocab.special_symbol2index[\"<eos>\"],\n","            )\n","        )\n","    return text_transform\n","\n","text_transform = get_text_transform(data_cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f69qgwbTd4u6"},"source":["print(text_transform[\"en\"](\"hello\"))\n","print(text_transform[\"en\"](\"hello,\"))\n","print(text_transform[\"en\"](\"hello, how\"))\n","print(text_transform[\"en\"](\"hello, how are you ?\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"15Y74ZhyeDmy"},"source":["# 4 collate_fn. --> batch를 전처리 할까? \n","def collate_fn(batch, data_cfg: DictConfig):\n","    src_batch, tgt_batch = [], []\n","\n","    for src_sample, tgt_sample in batch:\n","        src_batch.append(text_transform[data_cfg.src_lang](src_sample.rstrip(\"\\n\")))\n","        tgt_batch.append(text_transform[data_cfg.tgt_lang](tgt_sample.rstrip(\"\\n\")))\n","    \n","    src_batch = pad_sequence(src_batch, padding_value=data_cfg.vocab.special_symbol2index[\"<pad>\"])\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=data_cfg.vocab.special_symbol2index[\"<pad>\"])\n","    return src_batch, tgt_batch\n","\n","def get_collate_fn(cfg: DictConfig):\n","    return partial(collate_fn, data_cfg=cfg.data)\n","\n","# 5 data loader \n","def get_multi30k_dataloader(split_mode: str, language_pair: tuple, batch_size: int, collate_fn: Callable):\n","\n","    iter = Multi30k(split=split_mode, language_pair=language_pair)\n","    dataset = to_map_style_dataset(iter)\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n","    return dataloader\n","\n","test_dataloader = get_multi30k_dataloader(\"test\", (data_cfg.src_lang, data_cfg.tgt_lang), 3, collate_fn=partial(collate_fn, data_cfg=data_cfg))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kIwIoH9ElnlA"},"source":["for i in test_dataloader:\n","    print(i)\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJH1W_J4g-iN"},"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gyfV2e8iiawP"},"source":["def _text_postprocessing(res: List[str]) -> str:\n","    if \"<eos>\" in res:\n","        res = res[:res.index(\"<eos>\")]\n","    if \"<pad>\" in res:\n","        res = res[:res.index[\"<pad>\"]]\n","    res = \" \".join(res).replace(\"<bos>\", \"\")\n","    return res\n","\n","\n","class BaseTranslateLightningModule(pl.LightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.loss_function = torch.nn.CrossEntropyLoss(\n","            ignore_index=cfg.data.vocab.special_symbol2index[\"<pad>\"]\n","        )\n","    \n","    def configure_optimizers(self):\n","        self._optimizers, self._schedulers = configure_optimizers_from_cfg(\n","            self.cfg, self\n","        )\n","        return self._optimizers, self._schedulers\n","    \n","    @abstractmethod\n","    def forward(self, src, tgt, teacher_forcing_ratio: float):\n","        raise NotImplementedError()\n","    \n","    def _forward(self, src, tgt, mode: str, teacher_forcing_ratio: float = 0.5):\n","        # teacher forcing:\n","        # seq2seq 에서 많이 쓰인다.\n","        # src -> tgt autoregressive 학습하면, 맨 최초는 학습을 빠르게 한다. 근데, 미래부분 학습은? (앞부분 될때까지 기다리기 너무 힘들다...)\n","        # 랜덤으로 미래 정보도 조금 둬서 뒤에 있는 정보도 학습이 가능하게 하자 ! \n","        # 근데 0.5다? ==> 0.5 확률로 teacher_forcing을 하겠다 !\n","\n","        assert mode in [\"train\", \"val\", \"test\"]\n","\n","\n","        # get predictions\n","        # teacher_forcing 용 input --> \n","        tgt_inputs = tgt[:-1, :] # delete ends for teacher forcing inputs.\n","        outputs = self(src, tgt_inputs, teacher_forcing_ratio=teacher_forcing_ratio)\n","        tgt_outputs = tgt[1:, :] # delete start tokens.\n","\n","        loss = self.loss_function(\n","            outputs.reshape(-1, outputs.shape[-1]),  # [[batch X Seq_size], other_output_shape]\n","            tgt_outputs.reshape(-1),\n","        )\n","\n","        logs_detail = {\n","            f\"{mode}_src\": src,\n","            f\"{mode}_tgt\": tgt,\n","            f\"{mode}_results\": outputs,\n","        }\n","\n","        if mode in [\"val\", \"test\"]:\n","            _, tgt_results = torch.max(outputs, dim=2)\n","\n","            src_texts = []\n","            tgt_texts = []\n","            res_texts = []\n","\n","            # convert [L X B X others] --> [B X L X others]\n","            for src_i in torch.transpose(src, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.src_lang].lookup_tokens(src_i)\n","                src_texts.append(_text_postprocessing(res))\n","            \n","            for tgt_i in torch.transpose(tgt, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.tgt_lang].lookup_tokens(tgt_i)\n","                tgt_texts.append(_text_postprocessing(res))\n","            \n","            for tgt_res_i in torch.transpose(tgt_results, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.tgt_lang].lookup_tokens(tgt_res_i)\n","                res_texts.append(_text_postprocessing(res))\n","\n","            text_result_summary = {\n","                f\"{mode}_src_text\": src_texts,\n","                f\"{mode}_tgt_text\": tgt_texts,\n","                f\"{mode}_results_text\": res_texts,\n","            }\n","            print(f\"{self.global_step} step: \\n src_text: {src_texts[0]}, \\n tgt_text: {tgt_texts[0]}, \\n result_text: {res_texts[0]}\")\n","            logs_detail.update(text_result_summary)\n","\n","        return {f\"{mode}_loss\": loss}, logs_detail\n","    \n","\n","    def training_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, _ = self._forward(src, tgt, \"train\", self.cfg.model.teacher_forcing_ratio)\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"train_loss\"]\n","        return logs\n","    \n","    def validation_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, logs_detail = self._forward(src, tgt, \"val\", 0.0)\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"val_loss\"]\n","        logs.update(logs_detail)\n","        return logs\n","    \n","    def test_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, logs_detail = self._forward(src, tgt, \"test\", 0.0)\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"test_loss\"]\n","        logs.update(logs_detail)\n","        return logs\n","\n","\n","class TransformerTranslateLightningModule(BaseTranslateLightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__(cfg)\n","    \n","    @abstractmethod\n","    def forward(self, src, tgt):\n","        raise NotImplementedError()\n","    \n","    def _forward(self, src, tgt, mode: str):\n","        # teacher forcing:\n","        # seq2seq 에서 많이 쓰인다.\n","        # src -> tgt autoregressive 학습하면, 맨 최초는 학습을 빠르게 한다. 근데, 미래부분 학습은? (앞부분 될때까지 기다리기 너무 힘들다...)\n","        # 랜덤으로 미래 정보도 조금 둬서 뒤에 있는 정보도 학습이 가능하게 하자 ! \n","        # 근데 0.5다? ==> 0.5 확률로 teacher_forcing을 하겠다 !\n","\n","        assert mode in [\"train\", \"val\", \"test\"]\n","\n","\n","        # get predictions\n","        # teacher_forcing 용 input --> \n","        tgt_inputs = tgt[:-1, :] # delete ends\n","        outputs = self(src, tgt_inputs)\n","        tgt_outputs = tgt[1:, :] # delete start tokens.\n","\n","        loss = self.loss_function(\n","            outputs.reshape(-1, outputs.shape[-1]),  # [[batch X Seq_size], other_output_shape]\n","            tgt_outputs.reshape(-1),\n","        )\n","\n","        logs_detail = {\n","            f\"{mode}_src\": src,\n","            f\"{mode}_tgt\": tgt,\n","            f\"{mode}_results\": outputs,\n","        }\n","\n","        if mode in [\"val\", \"test\"]:\n","            _, tgt_results = torch.max(outputs, dim=2)\n","\n","            src_texts = []\n","            tgt_texts = []\n","            res_texts = []\n","\n","            # convert [L X B X others] --> [B X L X others]\n","            for src_i in torch.transpose(src, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.src_lang].lookup_tokens(src_i)\n","                src_texts.append(_text_postprocessing(res))\n","            \n","            for tgt_i in torch.transpose(tgt, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.tgt_lang].lookup_tokens(tgt_i)\n","                tgt_texts.append(_text_postprocessing(res))\n","            \n","            for tgt_res_i in torch.transpose(tgt_results, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.tgt_lang].lookup_tokens(tgt_res_i)\n","                res_texts.append(_text_postprocessing(res))\n","\n","            text_result_summary = {\n","                f\"{mode}_src_text\": src_texts,\n","                f\"{mode}_tgt_text\": tgt_texts,\n","                f\"{mode}_results_text\": res_texts,\n","            }\n","            print(f\"{self.global_step} step: \\n src_text: {src_texts[0]}, \\n tgt_text: {tgt_texts[0]}, \\n result_text: {res_texts[0]}\")\n","            logs_detail.update(text_result_summary)\n","\n","        return {f\"{mode}_loss\": loss}, logs_detail\n","    \n","\n","    def training_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, _ = self._forward(src, tgt, \"train\")\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"train_loss\"]\n","        return logs\n","    \n","    def validation_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, logs_detail = self._forward(src, tgt, \"val\")\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"val_loss\"]\n","        logs.update(logs_detail)\n","        return logs\n","    \n","    def test_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, logs_detail = self._forward(src, tgt, \"test\")\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"test_loss\"]\n","        logs.update(logs_detail)\n","        return logs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jpws4o6fnXTh"},"source":["# unils for initialization\n","def init_weights(model: Union[nn.Module, pl.LightningModule]):\n","    for name, param in model.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FE7ndfMMo7-2"},"source":["# model definition\n","\n","# 1. encoder (??)\n","class LSTMEncoder(nn.Module):\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        embed_dim: int,\n","        hidden_dim: int,\n","        n_layers: int,\n","        dropout: float\n","    ):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # initialization of weights\n","        self.apply(init_weights)\n","    \n","    def forward(self, src):\n","        # src = [seq_len, batch_size]\n","        embedded = self.dropout(self.embedding(src)) # [seq_len, batch_size, emb_dim]\n","\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","        # outputs = [seq_len, batch_size, hidden_dim * n directional]\n","        # hidden, cell = [n layers * n directions, batch_size, hidden_dim]\n","        \n","        # outputs will be used from top hidden layers. \n","        return hidden, cell\n","\n","# 2. decoder (??)\n","class LSTMDecoder(nn.Module):\n","    def __init__(\n","        self, \n","        output_dim: int,\n","        embed_dim: int,\n","        hidden_dim: int,\n","        n_layers: int,\n","        dropout: float\n","    ):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        \n","        self.output_dim = output_dim\n","        self.embedding = nn.Embedding(output_dim, embed_dim)\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout)\n","        self.fc_out = nn.Linear(hidden_dim, output_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","    \n","    def forward(self, input, hidden, cell):\n","        # input: [batch size  ...] < - start_token\n","\n","        # outputs = [seq_len, batch_size, hidden_dim * n directional]\n","        # hidden, cell = [n layers * 1 directions, batch_size, hidden_dim]\n","\n","        input = input.unsqueeze(0) # < - [1, batch_size]\n","        embedded = self.dropout(self.embedding(input))\n","\n","        # embedding = [1, batch_size, embed_dim]\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","\n","        # output = [1, batch_size, hidden_dim]\n","        # hidden, cell = [n layers * 1 directions, batch_size, hidden_dim]\n","\n","        prediction = self.fc_out(output.squeeze(0)) # [batch_size, output_dim] \n","\n","        return prediction, hidden, cell\n","\n","# 3. Seq2Seq(cfg) <-- encoder + decoder\n","class LSTMSeq2Seq(BaseTranslateLightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__(cfg)\n","\n","        self.encoder = LSTMEncoder(**cfg.model.enc)\n","        self.decoder = LSTMDecoder(**cfg.model.dec)\n","\n","        assert self.encoder.hidden_dim == self.decoder.hidden_dim\n","        assert self.encoder.n_layers == self.decoder.n_layers\n","\n","        # parameters 들 init.\n","        self.apply(init_weights)\n","    \n","    def forward(self, src, tgt, teacher_forcing_ratio: float = 0.5):\n","        \n","        # src, tgt = [seq_len (can be different), batch_size]\n","        # for val, test teacher forcing should be 0.0\n","\n","        batch_size = tgt.shape[1]\n","        tgt_len = tgt.shape[0]\n","        tgt_vocab_size = self.decoder.output_dim\n","\n","        # tensor to store decoder outputs\n","        outputs = torch.zeros(tgt_len, batch_size, tgt_vocab_size).to(self.device)\n","\n","        hidden, cell = self.encoder(src)\n","\n","        # start_token_input (<sos> tokens)\n","        input = tgt[0, :]\n","\n","        for t in range(1, tgt_len):\n","            \n","            # get one cell's output\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","\n","            # set to all outputs results\n","            outputs[t] = output\n","\n","            # decide whether going to use teacher forcing or not.\n","            teacher_force = random.random() < teacher_forcing_ratio\n","\n","            top1 = output.argmax(1)\n","\n","            input = tgt[t] if teacher_force else top1\n","\n","        return outputs\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-4Zpqm__tfi"},"source":["# Concat; Addictive Attention 기반의 모델 새로 정의.\n","# encoder, decoder rnn 이 다를 수 있다 ! \n","\n","class BidirectionalGRUEncoder(nn.Module):\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        embed_dim: int,\n","        enc_hidden_dim: int,\n","        dec_hidden_dim: int,\n","        n_layers: int,\n","        dropout: float\n","    ):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.n_layers = n_layers\n","\n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","        self.rnn = nn.GRU(embed_dim, enc_hidden_dim, n_layers, bidirectional=True, dropout=dropout)\n","        self.fc = nn.Linear(enc_hidden_dim * 2, dec_hidden_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # initialization of weights\n","        self.apply(init_weights)\n","    \n","    def forward(self, src):\n","\n","        # src = [seq_len, batch_size]\n","        embedded = self.dropout(self.embedding(src)) # [seq_len, batch_size, emb_dim]\n","\n","        outputs, hidden = self.rnn(embedded)\n","        # outputs = [seq_len, batch_size, hidden_dim * n directional]\n","        # hidden = [n layers * n directions, batch_size, hidden_dim]\n","\n","        # hidden -> [forward_1, backward_1, forward_2, backward_2, ..]\n","        # 우리가 필요한건 맨 마지막 layer의 forward backward 두개 concat 한개 필요.\n","        # => torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1) \n","        \n","        # encoder RNNs fed through a linear layer to connect decoder.\n","        hidden = torch.tanh(self.fc(\n","            torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n","        ))\n","\n","        return outputs, hidden\n","\n","\n","class ConcatAttention(nn.Module):\n","    def __init__(self, enc_hidden_dim: int, dec_hidden_dim: int):\n","        super().__init__()\n","        \n","        self.attn = nn.Linear((enc_hidden_dim * 2) + dec_hidden_dim, dec_hidden_dim)\n","        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)\n","    \n","    def forward(self, hidden, encoder_outputs):\n","        \n","        # hidden = [batch_size, dec_hidden_dim] => from decoder. (query)\n","        # encoder_outputs = [src_len, batch_size, enc_hidden_dim * 2] (key, value)\n","\n","        batch_size = encoder_outputs.shape[1]\n","        src_len = encoder_outputs.shape[0]\n","\n","        # repeat decoder hidden state src_len times ...\n","        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n","\n","        # hidden: [batch_size, src_len, dec_hidden_dim]\n","        # encoder_outputs = [barch_size, src_len, enc_hidden_dim * 2]\n","\n","        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n","\n","        # energy = [batch_size, src_len, dec_hidden_dim]\n","\n","        attention = self.v(energy).squeeze(2)\n","\n","        # attention = [batch_size, src_len]\n","\n","        return F.softmax(attention, dim=1)\n","\n","\n","class AttentionalRNNDecoder(nn.Module):\n","    def __init__(\n","        self,\n","        output_dim: int,\n","        embed_dim: int,\n","        enc_hidden_dim: int,\n","        dec_hidden_dim: int,\n","        n_layers: int,\n","        dropout: float,\n","        attention: nn.Module\n","    ):\n","        super().__init__()\n","\n","        self.output_dim = output_dim\n","        self.attention = attention\n","\n","        self.embedding = nn.Embedding(output_dim, embed_dim)\n","        self.rnn = nn.GRU((enc_hidden_dim * 2) + embed_dim, dec_hidden_dim, n_layers, dropout=dropout)\n","        self.fc_out = nn.Linear((enc_hidden_dim*2) + dec_hidden_dim + embed_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","    \n","    def forward(self, input, hidden, encoder_outputs):\n","\n","        # input [batch size] # start_token\n","        # hidden [batch_size, dec_hidden_dim]\n","        # encoder_outputs [src_len, batch_size, enc_hidden_dim*2]\n","\n","        input = input.unsqueeze(0) # input = [1, batch_size]\n","\n","        embedded = self.dropout(self.embedding(input)) # 1, batch_size, embed_dim\n","\n","        a = self.attention(hidden, encoder_outputs) # [batch_size, src_len]\n","        a = a.unsqueeze(1) # [batch_size, 1, src_len]\n","\n","        encoder_outputs = encoder_outputs.permute(1, 0, 2) # [batch_size, src_len, enc_hidden_dim*2]\n","        weighted = torch.bmm(a, encoder_outputs) # [batch_size, 1, enc_hidden_dim*2]\n","        weighted = weighted.permute(1, 0, 2) # [1, batch_size, enc_hidden_dim*2]\n","\n","        rnn_input = torch.cat((embedded, weighted), dim=2) # [1, batch_size, (enc_hidden_dim*2 + embed_dim)]\n","\n","        # hidden_unsqueeze: [1, batch_size, dec_hidden_dim]\n","        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n","        # output = [seq_len, batch_size, dec_hidden_dim * n directions] => [1, batch_size, dec_hidden_dim]\n","        # hidden = [n layers * n_directions, batch_size, dec_hidden_dims] => [1, batch_size, dec_hidden_dim]\n","\n","        if not (output == hidden).all():\n","            raise AssertionError()\n","        \n","        embedded = embedded.squeeze(0)\n","        output = output.squeeze(0)\n","        weighted = weighted.squeeze(0)\n","\n","        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1)) # [batch_size, output_dim]\n","\n","        return prediction, hidden.squeeze(0)\n","\n","\n","\n","class AttentionBasedSeq2Seq(BaseTranslateLightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__(cfg)\n","\n","        self.encoder = BidirectionalGRUEncoder(**cfg.model.enc)\n","        self.attention = ConcatAttention(**cfg.model.attention)\n","        self.decoder = AttentionalRNNDecoder(\n","            attention=self.attention, **cfg.model.dec\n","        )\n","\n","    def forward(self, src, tgt, teacher_forcing_ratio: float = 0.5):\n","        \n","        # src, tgt = [seq_len (can be different), batch_size]\n","        # for val, test teacher forcing should be 0.0\n","\n","        batch_size = tgt.shape[1]\n","        tgt_len = tgt.shape[0]\n","        tgt_vocab_size = self.decoder.output_dim\n","\n","        # tensor to store decoder outputs\n","        outputs = torch.zeros(tgt_len, batch_size, tgt_vocab_size).to(self.device)\n","\n","        encoder_outputs, hidden = self.encoder(src)\n","\n","        # start_token_input (<sos> tokens)\n","        input = tgt[0, :]\n","\n","        for t in range(1, tgt_len):\n","            \n","            # get one cell's output\n","            output, hidden = self.decoder(input, hidden, encoder_outputs)\n","\n","            # set to all outputs results\n","            outputs[t] = output\n","\n","            # decide whether going to use teacher forcing or not.\n","            teacher_force = random.random() < teacher_forcing_ratio\n","\n","            top1 = output.argmax(1)\n","\n","            input = tgt[t] if teacher_force else top1\n","\n","        return outputs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"enLTCXsg2R7C"},"source":["# 1. tokenembedding .. \n","# 2. positional encoding..\n","# 3. nn.Transformer ..\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(\n","        self,\n","        embed_size: int,\n","        dropout: float,\n","        maxlen: int = 5000\n","    ):\n","        super().__init__()\n","        den = torch.exp(-torch.arange(0, embed_size, 2)*math.log(10000) / embed_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        pos_embedding = torch.zeros((maxlen, embed_size))\n","        \n","        # sin: 2i\n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        # cos: 2i+1\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\"pos_embedding\", pos_embedding)\n","    \n","    def forward(self, token_embedding: torch.Tensor):\n","        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n","\n","\n","class TokenEmbedding(nn.Module):\n","    def __init__(\n","        self,\n","        vocab_size: int,\n","        embed_size: int\n","    ):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.embed_size = embed_size\n","\n","    def forward(self, tokens: torch.Tensor):\n","        return self.embedding(tokens.long()) * math.sqrt(self.embed_size) # scaling for embed size\n","\n","\n","class TransformerSeq2Seq(TransformerTranslateLightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__(cfg)\n","        self.cfg = cfg\n","        num_encoder_layers = self.cfg.model.num_encoder_layers\n","        num_decoder_layers = self.cfg.model.num_decoder_layers        \n","        embed_size = self.cfg.model.embed_size\n","        nhead = self.cfg.model.nhead\n","        src_vocab_size = self.cfg.model.src_vocab_size\n","        tgt_vocab_size = self.cfg.model.tgt_vocab_size\n","        dim_feedforward = self.cfg.model.dim_feedforward\n","        dropout = self.cfg.model.dropout\n","        \n","        self.transformer = Transformer(\n","            d_model=embed_size,\n","            nhead=nhead,\n","            num_encoder_layers=num_encoder_layers,\n","            num_decoder_layers=num_decoder_layers,\n","            dim_feedforward=dim_feedforward,\n","            dropout=dropout\n","        )\n","        self.generator = nn.Linear(embed_size, tgt_vocab_size)\n","        self.src_token_emb = TokenEmbedding(src_vocab_size, embed_size)\n","        self.tgt_token_emb = TokenEmbedding(tgt_vocab_size, embed_size)\n","        self.positional_encoding = PositionalEncoding(embed_size, dropout=dropout)\n","    \n","    def generate_square_subsequent_mask(self, sz: int):\n","        mask = (torch.triu(torch.ones((sz, sz), device=self.device)) == 1).transpose(0, 1)\n","        mask = mask.float().masked_fill(mask == 0, float(\"-inf\")).masked_fill(mask == 1, float(0.0))\n","        return mask\n","    \n","    def create_mask(self, src, tgt):\n","        src_seq_len = src.shape[0]\n","        tgt_seq_len = tgt.shape[0]\n","\n","        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n","        src_mask = torch.zeros((src_seq_len, src_seq_len), device=self.device).type(torch.bool)\n","\n","        src_padding_mask = (src == self.cfg.data.vocab.special_symbol2index[\"<pad>\"]).transpose(0, 1)\n","        tgt_padding_mask = (tgt == self.cfg.data.vocab.special_symbol2index[\"<pad>\"]).transpose(0, 1)\n","        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n","\n","    \n","    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, tgt)\n","        memory_key_padding_mask = src_padding_mask\n","        \n","        src_emb = self.positional_encoding(self.src_token_emb(src))\n","        tgt_emb = self.positional_encoding(self.tgt_token_emb(tgt))\n","        outs = self.transformer(\n","            src_emb, tgt_emb, src_mask, tgt_mask, None, src_padding_mask, tgt_padding_mask, memory_key_padding_mask\n","        )\n","        return self.generator(outs)\n","    \n","    def encode(self, src: torch.Tensor, src_mask: torch.Tensor):\n","        return self.transformer.encoder(self.positional_encoding(self.src_token_emb(src)), src_mask)\n","    \n","    def decode(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor):\n","        return self.transformer.decoder(self.positional_encoding(self.tgt_token_emb(tgt)), memory, tgt_mask)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dcrPfKy2p7cZ"},"source":["data_spacy_de_en_cfg = {\n","    \"name\": \"spacy_de_en\",\n","    \"data_root\": os.path.join(os.getcwd(), \"data\"),\n","    \"tokenizer\": \"spacy\",\n","    \"src_lang\": \"de\",\n","    \"tgt_lang\": \"en\",\n","    \"src_index\": 0,\n","    \"tgt_index\": 1,\n","    \"vocab\": {\n","        \"special_symbol2index\": {\n","            \"<unk>\": 0,\n","            \"<pad>\": 1,\n","            \"<bos>\": 2,\n","            \"<eos>\": 3,\n","        },\n","        \"special_first\": True,\n","        \"min_freq\": 2\n","    }\n","}\n","\n","data_cfg = OmegaConf.create(data_spacy_de_en_cfg)\n","\n","# get_dataset\n","train_data, valid_data, test_data = Multi30k(data_cfg.data_root)\n","\n","token_transform = get_token_transform(data_cfg)\n","vocab_transform = get_vocab_transform(data_cfg)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OV6EXifqww56"},"source":["# model configs\n","\n","model_translate_lstm_seq2seq_cfg = {\n","    \"name\": \"LSTMSeq2Seq\",\n","    \"enc\": {\n","        \"input_dim\": len(vocab_transform[data_cfg.src_lang]),\n","        \"embed_dim\": 256,\n","        \"hidden_dim\": 256,\n","        \"n_layers\": 2,\n","        \"dropout\": 0.5,\n","    },\n","    \"dec\": {\n","        \"output_dim\": len(vocab_transform[data_cfg.tgt_lang]),\n","        \"embed_dim\": 256,\n","        \"hidden_dim\": 256,\n","        \"n_layers\": 2,\n","        \"dropout\": 0.5,\n","    },\n","    \"teacher_forcing_ratio\": 0.5\n","}\n","\n","model_translate_attention_based_seq2seq_cfg = {\n","    \"name\": \"AttentionBasedSeq2Seq\",\n","    \"enc\": {\n","        \"input_dim\": len(vocab_transform[data_cfg.src_lang]),\n","        \"embed_dim\": 256,\n","        \"enc_hidden_dim\": 512,\n","        \"dec_hidden_dim\": 512,\n","        \"n_layers\": 1,\n","        \"dropout\": 0.5,\n","    },\n","    \"dec\": {\n","        \"output_dim\": len(vocab_transform[data_cfg.tgt_lang]),\n","        \"embed_dim\": 256,\n","        \"enc_hidden_dim\": 512,\n","        \"dec_hidden_dim\": 512,\n","        \"n_layers\": 1,\n","        \"dropout\": 0.5,\n","    },\n","    \"attention\": {\n","        \"enc_hidden_dim\": 512,\n","        \"dec_hidden_dim\": 512,\n","    },\n","    \"teacher_forcing_ratio\": 0.5\n","}\n","\n","model_translate_transformer_seq2seq_cfg = {\n","    \"name\": \"TransformerSeq2Seq\",\n","    \"num_encoder_layers\": 3,\n","    \"num_decoder_layers\": 3,\n","    \"embed_size\": 512,\n","    \"nhead\": 8,\n","    \"src_vocab_size\": len(vocab_transform[data_cfg.src_lang]),\n","    \"tgt_vocab_size\": len(vocab_transform[data_cfg.tgt_lang]),\n","    \"dim_feedforward\": 512,\n","    \"dropout\": 0.5,\n","}\n","\n","# opt_config\n","opt_cfg = {\n","    \"optimizers\": [\n","        {\n","            \"name\": \"RAdam\",\n","            \"kwargs\": {\n","                \"lr\": 1e-3,\n","            }\n","        }\n","    ],\n","    \"lr_schedulers\": [\n","        {\n","            \"name\": None,\n","            \"kwargs\": {\n","                \"warmup_end_steps\": 1000\n","            }\n","        }\n","    ]\n","}\n","\n","_merged_cfg_presets = {\n","    \"LSTM_seq2seq_de_en_translate\": {\n","        \"opt\": opt_cfg,\n","        \"data\": data_spacy_de_en_cfg,\n","        \"model\": model_translate_lstm_seq2seq_cfg,\n","    },\n","    \"attention_based_seq2seq_de_en_translate\": {\n","        \"opt\": opt_cfg,\n","        \"data\": data_spacy_de_en_cfg,\n","        \"model\": model_translate_attention_based_seq2seq_cfg,\n","    },\n","    \"transformer_seq2seq_de_en_translate\": {\n","        \"opt\": opt_cfg,\n","        \"data\": data_spacy_de_en_cfg,\n","        \"model\": model_translate_transformer_seq2seq_cfg,\n","    },\n","}\n","\n","# clear config hydra instance first\n","hydra.core.global_hydra.GlobalHydra.instance().clear()\n","\n","# register preset configs\n","register_config(_merged_cfg_presets)\n","\n","# initialization & compose configs\n","hydra.initialize(config_path=None)\n","cfg = hydra.compose(\"transformer_seq2seq_de_en_translate\")\n","\n","# overrride some cfg\n","run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{cfg.model.name}-{cfg.data.name}\"\n","\n","project_root_dir = os.path.join(\n","    drive_project_root, \"runs\", \"de_en_translate_tutorials\"\n",")\n","save_dir = os.path.join(project_root_dir, run_name)\n","run_root_dir = os.path.join(project_root_dir, run_name)\n","\n","train_cfg = {\n","    \"train_batch_size\": 128,\n","    \"val_batch_size\": 32,\n","    \"test_batch_size\": 32,\n","    \"train_val_split\": [0.9, 0.1],\n","    \"run_root_dir\": run_root_dir,\n","    \"trainer_kwargs\": {\n","        \"accelerator\": \"dp\",\n","        \"gpus\": \"0\",\n","        \"max_epochs\": 50,\n","        \"val_check_interval\": 1.0,\n","        \"log_every_n_steps\": 100,\n","        \"flush_logs_every_n_steps\": 100,\n","    }\n","}\n","\n","# logger config\n","log_cfg = {\n","    \"loggers\": {\n","        \"WandbLogger\": {\n","            \"project\": \"fastcampus_de_en_translate_tutorials\",\n","            \"name\": run_name,\n","            \"tags\": [\"fastcampus_de_en_translate_tutorials\"],\n","            \"save_dir\": run_root_dir,\n","        },\n","        \"TensorBoardLogger\": {\n","            \"save_dir\": project_root_dir,\n","            \"name\": run_name,\n","            \"log_graph\": True,\n","        }\n","    },\n","    \"callbacks\": {\n","        \"ModelCheckpoint\": {\n","            \"save_top_k\": 3,\n","            \"monitor\": \"val_loss\",\n","            \"mode\": \"min\",\n","            \"verbose\": True,\n","            \"dirpath\": os.path.join(run_root_dir, \"weights\"),\n","            \"filename\": \"{epoch}-{val_loss:.3f}\",\n","        },\n","        \"EarlyStopping\": {\n","            \"monitor\": \"val_loss\",\n","            \"mode\": \"min\",\n","            \"patience\": 3,\n","            \"verbose\": True,\n","        }\n","    }\n","}\n","\n","OmegaConf.set_struct(cfg, False)\n","cfg.train = train_cfg\n","cfg.log = log_cfg\n","\n","# lock config\n","OmegaConf.set_struct(cfg, True)\n","print(OmegaConf.to_yaml(cfg))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3q47TbWGyYwO"},"source":["# dataloader def\n","train_dataloader = get_multi30k_dataloader(\n","    \"train\",\n","    (data_cfg.src_lang, data_cfg.tgt_lang),\n","    cfg.train.train_batch_size,\n","    collate_fn=get_collate_fn(cfg)\n",")\n","val_dataloader = get_multi30k_dataloader(\n","    \"valid\",\n","    (data_cfg.src_lang, data_cfg.tgt_lang),\n","    cfg.train.val_batch_size,\n","    collate_fn=get_collate_fn(cfg)\n",")\n","test_dataloader = get_multi30k_dataloader(\n","    \"test\",\n","    (data_cfg.src_lang, data_cfg.tgt_lang),\n","    cfg.train.test_batch_size,\n","    collate_fn=get_collate_fn(cfg)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"npJ5p3hz0QpE"},"source":["# get model\n","def get_pl_model(cfg: DictConfig, checkpoint_path: Optional[str] = None):\n","\n","    if cfg.model.name == \"LSTMSeq2Seq\":\n","        model = LSTMSeq2Seq(cfg)\n","    elif cfg.model.name == \"AttentionBasedSeq2Seq\":\n","        model = AttentionBasedSeq2Seq(cfg)\n","    elif cfg.model.name == \"TransformerSeq2Seq\":\n","        model = TransformerSeq2Seq(cfg)\n","    else:\n","        raise NotImplementedError(\"Not implemented model\")\n","    \n","    if checkpoint_path is not None:\n","        model = model.load_from_checkpoint(cfg, checkpoint_path=checkpoint_path)\n","    return model\n","\n","model = None\n","model = get_pl_model(cfg)\n","print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DL6-s44L0yf2"},"source":["# pytorch lightning trainer def\n","logger = get_loggers(cfg)\n","callbacks = get_callbacks(cfg)\n","\n","trainer = pl.Trainer(\n","    callbacks=callbacks,\n","    logger=logger,\n","    default_root_dir=cfg.train.run_root_dir,\n","    num_sanity_val_steps=2,\n","    **cfg.train.trainer_kwargs,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"chRbuwfu1i5z"},"source":["trainer.fit(model, train_dataloader, val_dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X8ODG_SW33Mg"},"source":[""],"execution_count":null,"outputs":[]}]}