{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJT5kqyZGe1p"
   },
   "source": [
    "# Multi-layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzHP2rndF-0G"
   },
   "source": [
    "## 외부 파일 가져오기 & requirements 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mi95dewjGeW_"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "sys.path.append(\"/content/drive/MyDrive/#fastcampus\")\n",
    "!pip install -r \"/content/drive/MyDrive/#fastcampus/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YD7vyST9GdHv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch_optimizer import RAdam\n",
    "from torch_optimizer import AdamP\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QX3soatAMk5B"
   },
   "outputs": [],
   "source": [
    "from data_utils import dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lISGUqfUG_Ny"
   },
   "outputs": [],
   "source": [
    "data_root = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "# 전처리 부분 (preprocessing) & 데이터 셋 정의.\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]), # mean, # std\n",
    "    ]\n",
    ")\n",
    "fashion_mnist_dataset = FashionMNIST(data_root, download=True, train=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0waoPp23NcE0"
   },
   "source": [
    "## Dataloader를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5Nq1tZiJ84g"
   },
   "outputs": [],
   "source": [
    "datasets = dataset_split(fashion_mnist_dataset, split=[0.9, 0.1])\n",
    "\n",
    "train_dataset = datasets[\"train\"]\n",
    "val_dataset = datasets[\"val\"]\n",
    "\n",
    "train_batch_size = 100\n",
    "val_batch_size = 10\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=1\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=val_batch_size, shuffle=False, num_workers=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKRCanL6LtoM"
   },
   "outputs": [],
   "source": [
    "for sample_batch in train_dataloader:\n",
    "    print(sample_batch)\n",
    "    print(sample_batch[0].shape, sample_batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lwd5nW_3N37z"
   },
   "source": [
    "## 모델 (Multi-layer Perceptron) (MLP) ! 정의\n",
    "## 모델 MLPWithDropout 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-O--VMLMN04F"
   },
   "outputs": [],
   "source": [
    "# Define Model.\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, h1_dim: int, h2_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_dim, h1_dim)\n",
    "        self.linear2 = nn.Linear(h1_dim, h2_dim)\n",
    "        self.linear3 = nn.Linear(h2_dim, out_dim)\n",
    "        self.relu = F.relu\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = torch.flatten(input, start_dim=1)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        out = self.linear3(x)\n",
    "        # out = F.softmax(out)\n",
    "        return out\n",
    "\n",
    "class MLPWithDropout(MLP):\n",
    "    def __init__(self, in_dim: int, h1_dim: int, h2_dim: int, out_dim: int, dropout_prob: float):\n",
    "        super().__init__(in_dim, h1_dim, h2_dim, out_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = torch.flatten(input, start_dim=1)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.dropout2(x)\n",
    "        out = self.linear3(x)\n",
    "        # out = F.softmax(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMM1y75wql8f"
   },
   "source": [
    "## Learning Rate Scheduler \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3y1m5T3-qqCk"
   },
   "outputs": [],
   "source": [
    "# Warmup Scheduler\n",
    "class WarmupLR(optim.lr_scheduler.LambdaLR):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: optim.Optimizer,\n",
    "        warmup_end_steps: int,\n",
    "        last_epoch: int = -1,\n",
    "    ):\n",
    "        \n",
    "        def wramup_fn(step: int):\n",
    "            if step < warmup_end_steps:\n",
    "                return float(step) / float(max(warmup_end_steps, 1))\n",
    "            return 1.0\n",
    "        \n",
    "        super().__init__(optimizer, wramup_fn, last_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8tljBSaQBZB"
   },
   "source": [
    "## 모델 선언 및 손실 함수, 최적화(Optimizer) 정의, Tensorboard Logger 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8p_hsjsO-on"
   },
   "outputs": [],
   "source": [
    "# define model.\n",
    "# model = MLP(28*28, 128, 64, 10)\n",
    "model = MLPWithDropout(28*28, 128, 64, 10, dropout_prob=0.3)\n",
    "model_name = type(model).__name__\n",
    "\n",
    "# define loss\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# define optimizer\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = RAdam(model.parameters(), lr=lr)\n",
    "# optimizer = AdamP(model.parameters(), lr=lr)\n",
    "optimizer_name = type(optimizer).__name__\n",
    "\n",
    "# define scheduler\n",
    "scheduler = WarmupLR(optimizer, 1500)\n",
    "scheduler_name = type(scheduler).__name__ if scheduler is not None else \"no\"\n",
    "\n",
    "max_epoch = 50\n",
    "\n",
    "# define tensorboard logger\n",
    "run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{model_name}-{optimizer_name}_optim_{lr}_lr_with_{scheduler_name}_scheduler\"\n",
    "log_dir = f\"runs/{run_name}\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "log_interval = 100\n",
    "\n",
    "# define wandb\n",
    "project_name = \"fastcampus_fashion_mnist_tutorials\"\n",
    "run_tags = [project_name]\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    name=run_name,\n",
    "    tags=run_tags,\n",
    "    config={\"lr\": lr, \"model_name\": model_name, \"optimizer_name\": optimizer_name, \"scheduler_name\": scheduler_name},\n",
    "    reinit=True,\n",
    ")\n",
    "\n",
    "# set save model path\n",
    "log_model_path = os.path.join(log_dir, \"models\")\n",
    "os.makedirs(log_model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vv-5yJgesU01"
   },
   "outputs": [],
   "source": [
    "# for i in range(250):\n",
    "#     print(\"step\", i)\n",
    "#     optimizer.step()\n",
    "#     scheduler.step()\n",
    "#     print(scheduler.get_last_lr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnGxA90KyyUi"
   },
   "source": [
    "## Early Stopping callback Object Class 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyuzST93ynLE"
   },
   "outputs": [],
   "source": [
    "# With some modifications, source is from https://github.com/Bjarten/early-stopping-pytorch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.ckpt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.ckpt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        \n",
    "        filename = self.path.split('/')[-1]\n",
    "        save_dir = os.path.dirname(self.path)\n",
    "        torch.save(model, os.path.join(save_dir, f\"val_loss-{val_loss}-{filename}\"))\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oq-XpcGXQu1v"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/\n",
    "\n",
    "# define EarlyStopping.\n",
    "early_stopper = EarlyStopping(\n",
    "    patience=3, verbose=True, path=os.path.join(log_model_path, \"model.ckpt\")\n",
    ")\n",
    "\n",
    "# do train with validation.\n",
    "train_step = 0\n",
    "for epoch in range(1, max_epoch+1):\n",
    "    # valid step\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        model.eval()\n",
    "\n",
    "        for val_batch_idx, (val_images, val_labels) in enumerate(\n",
    "            tqdm(val_dataloader, position=0, leave=True, desc=\"validation\")\n",
    "        ):\n",
    "            # forward\n",
    "            val_outputs = model(val_images)\n",
    "            _, val_preds = torch.max(val_outputs, 1)\n",
    "            \n",
    "            # loss & acc\n",
    "            val_loss += loss_function(val_outputs, val_labels) / val_outputs.shape[0]\n",
    "            val_corrects += torch.sum(val_preds == val_labels.data) / val_outputs.shape[0]\n",
    "    \n",
    "    # valid step logging\n",
    "    val_epoch_loss = val_loss / len(val_dataloader)\n",
    "    val_epoch_acc = val_corrects / len(val_dataloader)\n",
    "    \n",
    "    print(\n",
    "        f\"{epoch} epoch, {train_step} step: val_loss: {val_epoch_loss}, val_acc: {val_epoch_acc}\"\n",
    "    )\n",
    "\n",
    "    # tensorboard log\n",
    "    writer.add_scalar(\"Loss/val\", val_epoch_loss, train_step)\n",
    "    writer.add_scalar(\"Acc/val\", val_epoch_acc, train_step)\n",
    "    writer.add_images(\"Images/val\", val_images, train_step)\n",
    "\n",
    "    # wandb log\n",
    "    wandb.log({\n",
    "        \"Loss/val\": val_epoch_loss,\n",
    "        \"Acc/val\": val_epoch_acc,\n",
    "        \"Images/val\": wandb.Image(val_images),\n",
    "        \"Outputs/val\": wandb.Histogram(val_outputs.detach().numpy()),\n",
    "        \"Preds/val\": wandb.Histogram(val_preds.detach().numpy()),\n",
    "        \"Labels/val\": wandb.Histogram(val_labels.data.detach().numpy()),\n",
    "    }, step=train_step)\n",
    "\n",
    "    # check model early stopping point & save model if the model reached the best performance.\n",
    "    early_stopper(val_epoch_loss, model)\n",
    "    if early_stopper.early_stop:\n",
    "        break\n",
    "    \n",
    "    # train step\n",
    "    current_loss = 0\n",
    "    current_corrects = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(\n",
    "         tqdm(train_dataloader, position=0, leave=True, desc=\"training\")\n",
    "    ):\n",
    "        current_loss = 0.0\n",
    "        current_corrects = 0\n",
    "\n",
    "        # Forward\n",
    "        # get predictions\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # get loss (Loss 계산)\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        # optimizer 초기화 (zero화)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform Optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Perform LR scheduler Work\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        current_loss += loss.item()\n",
    "        current_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        if train_step % log_interval == 0:\n",
    "            train_loss = current_loss / log_interval\n",
    "            train_acc = current_corrects / log_interval\n",
    "\n",
    "            print(\n",
    "                f\"{train_step}: train_loss: {train_loss}, train_acc: {train_acc}\"\n",
    "            )\n",
    "            # tensorboard log\n",
    "            writer.add_scalar(\"Loss/train\", train_loss, train_step)\n",
    "            writer.add_scalar(\"Acc/train\", train_acc, train_step)\n",
    "            writer.add_images(\"Images/train\", images, train_step)\n",
    "            writer.add_scalar(\"Learning Rate\", scheduler.get_last_lr()[0], train_step)\n",
    "            writer.add_graph(model, images)\n",
    "\n",
    "            # wandb log\n",
    "            wandb.log({\n",
    "                \"Loss/train\": train_loss,\n",
    "                \"Acc/train\": train_acc,\n",
    "                \"Images/train\": wandb.Image(images),\n",
    "                \"Outputs/train\": wandb.Histogram(outputs.detach().numpy()),\n",
    "                \"Preds/train\": wandb.Histogram(preds.detach().numpy()),\n",
    "                \"Labels/train\": wandb.Histogram(labels.data.detach().numpy()),\n",
    "                \"Learning Rate\": scheduler.get_last_lr()[0],\n",
    "            }, step=train_step)\n",
    "\n",
    "            current_loss = 0\n",
    "            current_corrects = 0\n",
    "\n",
    "        train_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymnDN1VkYOvt"
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "loaded_model = torch.load(os.path.join(log_model_path, \"val_loss-0.031634580343961716-model.ckpt\"))\n",
    "loaded_model.eval()\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHvB9xAnYWBk"
   },
   "outputs": [],
   "source": [
    "def softmax(x, axis=0):\n",
    "    \"numpy softmax\"\n",
    "    max = np.max(x, axis=axis, keepdims=True)\n",
    "    e_x = np.exp(x - max)\n",
    "    sum = np.sum(e_x, axis=axis, keepdims=True)\n",
    "    f_x = e_x / sum\n",
    "    return f_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UoTf2ztY7nT"
   },
   "outputs": [],
   "source": [
    "test_batch_size = 100\n",
    "test_dataset = FashionMNIST(data_root, download=True, train=False, transform=transforms.ToTensor())\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "test_labels_list = []\n",
    "test_preds_list = []\n",
    "test_outputs_list = []\n",
    "for i, (test_images, test_labels) in enumerate(tqdm(test_dataloader, position=0, leave=True, desc=\"testing\")):\n",
    "    # forward\n",
    "    test_outputs = loaded_model(test_images)\n",
    "    _, test_preds = torch.max(test_outputs, 1)\n",
    "\n",
    "    final_outs = softmax(test_outputs.detach().numpy(), axis=1)\n",
    "    test_outputs_list.extend(final_outs)\n",
    "    test_preds_list.extend(test_preds.detach().numpy())\n",
    "    test_labels_list.extend(test_labels.detach().numpy())\n",
    "\n",
    "test_preds_list = np.array(test_preds_list)\n",
    "test_labels_list = np.array(test_labels_list)\n",
    "\n",
    "print(f\"\\nacc: {np.mean(test_preds_list == test_labels_list)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0a4PPefaRfH"
   },
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh = {}\n",
    "n_class = 10\n",
    "\n",
    "for i in range(n_class):\n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(test_labels_list, np.array(test_outputs_list)[:, i], pos_label=i)\n",
    "\n",
    "# plot.\n",
    "for i in range(n_class):\n",
    "    plt.plot(fpr[i], tpr[i], linestyle=\"--\", label=f\"Class {i} vs Rest\")\n",
    "plt.title(\"Multi-class ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "print(\"auc_score\", roc_auc_score(test_labels_list, test_outputs_list, multi_class=\"ovo\", average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dm5jZ7urbMAO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNb0fHuV1Q839JZ8EbTucQa",
   "collapsed_sections": [],
   "mount_file_id": "1gNh-HsjMOM_aMXda7EfQdjZjqAA3DNqt",
   "name": "04. MLP + Optimizers.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1n37-PsBNU1tEXNwUvWohvHVJD81-C8m2",
     "timestamp": 1627212638626
    },
    {
     "file_id": "1gNh-HsjMOM_aMXda7EfQdjZjqAA3DNqt",
     "timestamp": 1626696451891
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
